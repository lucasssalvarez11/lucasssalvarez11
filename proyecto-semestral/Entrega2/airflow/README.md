# SodAI Drinks ü•§ ‚Äì Pipeline productivo (Airflow)

Este directorio contiene el **pipeline productivo** para SodAI Drinks, orquestado con **Airflow**.  
El objetivo es automatizar el flujo completo:

1. Extracci√≥n y preparaci√≥n de datos
2. Construcci√≥n de dataset cliente‚Äìproducto‚Äìsemana
3. Preprocesamiento (dummies, escalado, features adicionales)
4. Detecci√≥n de drift de datos
5. Reentrenamiento condicional del modelo (XGBoost + Optuna)
6. Persistencia de artefactos (modelo y preprocesador) para ser usados por la aplicaci√≥n (`app/backend`)

---

## 1. Estructura de carpetas

```text
airflow/
  dags/
    helper_functions.py   # L√≥gica de negocio y funciones auxiliares de ML
    pipeline.py           # Definici√≥n del DAG de Airflow
  data/
    raw/                  # Datos de entrada (parquet).
    processed/            # Datasets procesados y splits.
    models/               # Artefactos del modelo.
    predictions/          # (Opcional) Salidas de predicci√≥n.

## 2. Orquestaci√≥n del pipeline con Airflow

Este script define un **DAG de Airflow** llamado `weekly_ml_pipeline` que ejecuta de forma orquestada el pipeline de ML:

1. Preparaci√≥n de directorios.
2. Extracci√≥n de datos.
3. Preprocesamiento y generaci√≥n de splits.
4. Detecci√≥n de drift.
5. Reentrenamiento condicional del modelo XGBoost usando Optuna (solo si hay drift).

El DAG est√° agendado para correrse de forma **semanal** (`schedule_interval="@weekly"`) y no hace *catchup* de fechas pasadas.

### Funciones principales (callables)

- `drift_branch_callable(**kwargs)`
  - Ejecuta `run_drift_detection(threshold=0.1)`.
  - Imprime el resultado de la detecci√≥n de drift.
  - Devuelve el `task_id` al que debe ir el flujo:
    - `"train_xgboost_with_optuna"` si **se detecta drift**.
    - `"skip_training"` si **no hay drift**.
  - Esta funci√≥n se usa en un `BranchPythonOperator` para decidir din√°micamente el camino del DAG.

- `train_with_optuna_callable()`
  - Lanza la optimizaci√≥n de hiperpar√°metros con `run_optuna_tuning(n_trials=30)`.
  - Usa los mejores par√°metros encontrados para entrenar el modelo final llamando a `train_model(best_params=best_params)`.
  - Imprime las m√©tricas finales del modelo en el set de test.

Ambas funciones consumen las funciones definidas en `helper_functions.py`:
`ensure_dirs`, `run_extraction`, `run_preprocessing`, `run_drift_detection`, `run_optuna_tuning`, `train_model`.

### Estructura del DAG y tareas

El DAG se define con:

- `dag_id="weekly_ml_pipeline"`
- `schedule_interval="@weekly"`
- `start_date=datetime(2024, 1, 1)`
- `tags=["ml", "xgboost", "optuna", "drift"]`

Las tareas son:

- `start` (`EmptyOperator`): nodo inicial del flujo.
- `ensure_dirs` (`PythonOperator`):
  - Llama a `ensure_dirs()` para asegurarse de que existan las carpetas base (`data/`, `raw`, `processed`, `models`, etc.).
- `extract_data` (`PythonOperator`):
  - Ejecuta `run_extraction()` para leer los datos crudos, hacer ajustes b√°sicos y guardarlos en `data/processed/`.
- `preprocess_data` (`PythonOperator`):
  - Llama a `run_preprocessing()` para construir el dataset base, aplicar preprocesamiento y generar los splits temporales `train/val/test`.
- `check_drift_and_branch` (`BranchPythonOperator`):
  - Ejecuta `drift_branch_callable()` y, seg√∫n el resultado, bifurca el flujo hacia:
    - `train_xgboost_with_optuna` (si hay drift).
    - `skip_training` (si no hay drift).
- `train_xgboost_with_optuna` (`PythonOperator`):
  - Ejecuta `train_with_optuna_callable()`, que corre Optuna + entrenamiento final del modelo XGBoost.
- `skip_training` (`EmptyOperator`):
  - Nodo ‚Äúdummy‚Äù que representa el camino donde se decide **no reentrenar** el modelo (no hay drift relevante).
- `end` (`EmptyOperator`):
  - Nodo final al que convergen ambos caminos (con o sin reentrenamiento).

### Flujo de dependencias

El flujo general del DAG es:

```text
start
  ‚Üì
ensure_dirs
  ‚Üì
extract_data
  ‚Üì
preprocess_data
  ‚Üì
check_drift_and_branch
        ‚Üô               ‚Üò
train_xgboost_with_optuna   skip_training
        ‚Üò               ‚Üô
             end


## 2. Pipeline de modelado (extracci√≥n, features, entrenamiento y drift)

Este script implementa el **pipeline completo** para un problema de clasificaci√≥n binaria de compra (`target`) a nivel `cliente-producto-semana`.  
Incluye:

- Lectura y preparaci√≥n de datos crudos (`.parquet`).
- Construcci√≥n del dataset base (nivel cliente‚Äìproducto‚ÄìA√±o‚ÄìSemana).
- Preprocesamiento (imputaci√≥n, dummies y escalado).
- Split temporal train/val/test.
- Optimizaci√≥n de hiperpar√°metros y entrenamiento de un modelo **XGBoost**.
- Detecci√≥n de **data drift** entre distintos datasets.

La estructura de carpetas asumida es:

- `data/raw/` ‚Üí datos crudos:
  - `transacciones.parquet`
  - `clientes.parquet`
  - `productos.parquet`
- `data/processed/` ‚Üí datos procesados e intermedios.
- `data/models/` ‚Üí modelos y objetos serializados.
- `data/predictions/` ‚Üí (reservado para salidas de predicci√≥n).

### Flujo general del pipeline

1. **Extracci√≥n de datos crudos**
   - `run_extraction()`
     - Lee los tres archivos de origen desde `data/raw/`.
     - En la tabla de transacciones, renombra la columna `items` a `payment` si corresponde.
     - Guarda las tres tablas ‚Äúlimpias‚Äù en `data/processed/` para el siguiente paso.

2. **Construcci√≥n del DataFrame base (cliente‚Äìproducto‚Äìsemana)**
   - `_load_processed_raws()`
     - Carga las versiones procesadas de `transacciones`, `clientes` y `productos` desde `data/processed/`.
   - `build_base_df(df_trans, df_clientes, df_productos)`
     - Limpia tipos de datos (por ejemplo, columnas categ√≥ricas como `string`).
     - Convierte `purchase_date` a fecha y asegura que `payment` sea num√©rica.
     - Agrega transacciones por orden para evitar duplicados.
     - Calcula **A√±o** y **Semana ISO** a partir de la fecha.
     - Define la variable objetivo `target = 1` si hubo compra (payment > 0) en esa combinaci√≥n `cliente-producto-A√±o-Semana`, y `0` en caso contrario.
     - Genera **todas las combinaciones posibles** de `cliente-producto-A√±o-Semana` observadas en los datos y completa con:
       - `total_payment`, `n_orders` y `target`.
     - Hace *join* con las tablas de clientes y productos para incorporar sus atributos como features.

3. **Preprocesamiento de datos (dummies + escalado)**
   - `preprocess_df(df)`
     - Revisa que existan las columnas de ID (`customer_id`, `product_id`, `A√±o`, `Semana`) y `target`.
     - Separa:
       - **Features num√©ricas**: columnas num√©ricas distintas de los IDs y del `target`.
       - **Features categ√≥ricas**: columnas de tipo `object`/`string` distintas de IDs y `target`.
     - Aplica un `ColumnTransformer` con dos pipelines:
       - Num√©ricas: `SimpleImputer(strategy="median")` + `StandardScaler()`.
       - Categ√≥ricas: `SimpleImputer(strategy="most_frequent")` + `OneHotEncoder(handle_unknown="ignore")`.
     - Devuelve un `df_final` que contiene:
       - Columnas de ID + `target`.
       - Todas las features transformadas (incluyendo dummies).
     - Guarda el DataFrame preprocesado en `data/processed/df_preprocesado.parquet`.
     - Serializa el `preprocessor` (ColumnTransformer) en `data/models/preprocessor.pkl`.

   - `run_preprocessing()`
     - Funci√≥n ‚Äúde alto nivel‚Äù que ejecuta:
       1. Carga de datos procesados (`_load_processed_raws`).
       2. Construcci√≥n del DF base (`build_base_df`).
       3. Preprocesamiento (`preprocess_df`).
       4. **Split temporal** 70/15/15 (train/val/test) v√≠a `temporal_split`.
     - Devuelve el DataFrame preprocesado completo.

4. **Split temporal (train / val / test)**
   - `temporal_split(df, train_frac=0.7, val_frac=0.15)`
     - Crea una fecha de referencia a partir de `A√±o` y `Semana` (lunes de cada semana).
     - Ordena el dataset por esta fecha.
     - Separa en:
       - 70% filas iniciales ‚Üí `train.parquet`
       - 15% siguientes ‚Üí `val.parquet`
       - 15% finales ‚Üí `test.parquet`
     - Guarda estos 3 conjuntos en `data/processed/`.

   - `_load_splits()`
     - Carga `train.parquet`, `val.parquet` y `test.parquet` desde `data/processed/`.

   - `_get_feature_cols(df)`
     - Devuelve la lista de columnas de features (es decir, todas menos las columnas de ID y la columna objetivo `target`).

5. **Optimizaci√≥n de hiperpar√°metros y entrenamiento de XGBoost**
   - `run_optuna_tuning(n_trials=30)`
     - Carga los splits `train` y `val`.
     - Define un espacio de b√∫squeda de hiperpar√°metros para `xgboost.XGBClassifier` (profundidad, learning rate, n_estimators, subsample, regularizaciones, etc.).
     - Usa **Optuna** para maximizar el **F1-macro** en el set de validaci√≥n.
     - Devuelve el diccionario `best_params` con los mejores hiperpar√°metros encontrados.

   - `train_model(best_params: Dict[str, Any] | None = None)`
     - Carga train, val y test.
     - Si `best_params` es `None`, llama internamente a `run_optuna_tuning()`.
     - Combina `train` + `val` para entrenar el modelo final con m√°s datos.
     - Entrena un `XGBClassifier` con los par√°metros base + `best_params`.
     - Eval√∫a en el set de test:
       - `F1-macro`
       - `Accuracy`
     - Guarda el modelo entrenado en `data/models/xgb_model.pkl` junto con la lista de columnas de features.
     - Retorna un diccionario con las m√©tricas de test.

6. **Detecci√≥n de data drift**
   - `detect_drift(df_old, df_new, threshold=0.1)`
     - Compara la distribuci√≥n de las columnas comunes entre dos datasets:
       - Para columnas num√©ricas (con suficiente cardinalidad) aplica **Kolmogorov‚ÄìSmirnov (KS)** y usa el *p-value* como m√©trica de similitud.
       - Para columnas categ√≥ricas calcula la distancia **Jensen‚ÄìShannon** entre las distribuciones de frecuencias y utiliza `1 - js` como similitud.
     - Calcula un `avg_score` promedio de similitud entre todas las columnas.
     - Define que hay drift (`drift_detected = True`) si `avg_score` es menor que el umbral (`threshold`).
     - Devuelve:
       - `{"drift_detected": bool, "avg_score": float}`.

   - `run_drift_detection(threshold=0.1)`
     - Usa por defecto:
       - `train.parquet` como dataset de referencia (`df_old`).
       - `test.parquet` como ‚Äúnuevo‚Äù dataset (`df_new`).
     - Excluye las columnas de ID y `target` (solo analiza features).
     - Llama a `detect_drift` y devuelve el resultado.

### Funciones utilitarias

- `ensure_dirs()`
  - Se asegura de que existan las rutas base (`data/`, `data/raw/`, `data/processed/`, `data/models/`, `data/predictions/`), cre√°ndolas si no existen.


