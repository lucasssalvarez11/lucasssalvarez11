{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyPTffTLug7i"
      },
      "source": [
        "\n",
        "\n",
        "# **Laboratorio 11: Pienso, luego predigo üí°**\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2025</strong></center>\n",
        "\n",
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Diego Cortez, Gabriel Iturra\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy6ikgVYzghB"
      },
      "source": [
        "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados**\n",
        "\n",
        "- Nombre de alumno 1: Lucas Alvarez Pezoa\n",
        "- Nombre de alumno 2: Benjamin Villaseca Vicu√±a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMJ-owchzjFf"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/...../)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUuwsXrKzmkK"
      },
      "source": [
        "## **Temas a tratar**\n",
        "\n",
        "- Reinforcement Learning\n",
        "- Large Language Models\n",
        "\n",
        "## **Reglas:**\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda **fuertemente** asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### **Objetivos principales del laboratorio**\n",
        "\n",
        "- Resoluci√≥n de problemas secuenciales usando Reinforcement Learning\n",
        "- Habilitar un Chatbot para entregar respuestas √∫tiles usando Large Language Models.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hmHHQ9BuyAG"
      },
      "source": [
        "## **1. Reinforcement Learning (2.0 puntos)**\n",
        "\n",
        "En esta secci√≥n van a usar m√©todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gOcejYb6uzOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4fdec8-30d2-4080-bc77-c510ecb052e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qqq gymnasium stable_baselines3\n",
        "!pip install -qqq swig\n",
        "!pip install -qqq gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBPet_Mq8dX9"
      },
      "source": [
        "### **1.1 Blackjack (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "La idea de esta subsecci√≥n es que puedan implementar m√©todos de RL y as√≠ generar una estrategia para jugar el cl√°sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
        "\n",
        "Comencemos primero preparando el ambiente. El siguiente bloque de c√≥digo transforma las observaciones del ambiente a `np.array`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LpZ8bBKk9ZlU"
      },
      "outputs": [],
      "source": [
        " import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "import numpy as np\n",
        "\n",
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ6J1_-Y9nHO"
      },
      "source": [
        "#### **1.1.1 Descripci√≥n de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripci√≥n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5i1Wt1p770x"
      },
      "source": [
        "El ambiente Blackjack-v1 modela una mano de blackjack como un MDP epis√≥dico: cada episodio corresponde a una mano, que termina cuando el jugador decide plantarse o se pasa de 21. El estado se representa como una tupla (player_sum, dealer_card, usable_ace). Aqu√≠, player_sum es la suma actual de las cartas del jugador, dealer_card es el valor de la carta visible del crupier (entre 1 y 10) y usable_ace indica si el jugador tiene un As que puede contarse como 11 sin pasar de 21 (True/False). En tu wrapper esto se codifica como un arreglo de numpy con espacio MultiDiscrete([32, 11, 2]).\n",
        "\n",
        "El conjunto de acciones es: pedir carta (hit, acci√≥n 1) o plantarse (stick, acci√≥n 0). Si el agente pide carta, recibe una nueva carta y, si con eso su suma supera 21, el episodio termina inmediatamente. Si se planta, deja de recibir cartas, el crupier juega siguiendo una regla fija (pedir hasta tener al menos 17 puntos) y luego se comparan las manos para decidir el resultado.\n",
        "\n",
        "La funci√≥n de recompensas est√° definida solo al final del episodio. El jugador recibe +1 si gana la mano, ‚àí1 si pierde (porque se pas√≥ o qued√≥ con menos puntaje que el crupier) y 0 si empata. Durante los pasos intermedios, mientras la mano sigue en juego, la recompensa es 0. Bajo esta formulaci√≥n MDP, el objetivo del agente de aprendizaje por refuerzo es encontrar una pol√≠tica œÄ(a | s) que maximice la recompensa esperada por episodio, es decir, que le permita jugar al blackjack de la forma m√°s rentable posible a largo plazo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmcX6bRC9agQ"
      },
      "source": [
        "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 5000 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica? ¬øC√≥mo podr√≠a interpretar las recompensas obtenidas?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.spaces import MultiDiscrete\n",
        "import numpy as np\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "import imageio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKgjbSLMhj0W",
        "outputId": "22b831c0-7b66-42a1-da63-669442210004"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9p2PrLLR9yju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4312ac9c-d07b-4ca1-e40b-68dd295eafea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -0.3846\n",
            "Desviaci√≥n est√°ndar: 0.9019328356368893\n"
          ]
        }
      ],
      "source": [
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)\n",
        "\n",
        "n_episodios = 5000\n",
        "recompensas = []\n",
        "\n",
        "for _ in range(n_episodios):\n",
        "    obs, info = env.reset()\n",
        "    terminado = False\n",
        "    truncado = False\n",
        "    retorno = 0.0\n",
        "\n",
        "    while not (terminado or truncado):\n",
        "        accion = env.action_space.sample()\n",
        "        obs, reward, terminado, truncado, info = env.step(accion)\n",
        "        retorno += reward\n",
        "\n",
        "    recompensas.append(retorno)\n",
        "\n",
        "recompensas = np.array(recompensas)\n",
        "promedio = recompensas.mean()\n",
        "desviacion = recompensas.std()\n",
        "\n",
        "print(\"Promedio de recompensas:\", promedio)\n",
        "print(\"Desviaci√≥n est√°ndar:\", desviacion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?\n",
        "\n",
        "La pol√≠tica es claramente mala. Un promedio de 0.3836 negativo significa que, en promedio, est√°s perdiendo 0.3836 unidades de apuesta por cada mano jugada cuando eliges las acciones al azar. Si esto fuera dinero real, cada vez que juegas una mano con esta estrategia esperas perder cerca de un 38 % de tu apuesta, lo cual es bastante malo. Entonces, como baseline, esta pol√≠tica sirve para mostrar qu√© tan mal se juega si no usas ninguna informaci√≥n del estado, pero no ser√≠a una estrategia razonable en un casino.\n",
        "\n",
        "¬øC√≥mo interpretar las recompensas obtenidas?\n",
        "\n",
        "Al tomar muchas manos (5000 episodios) y calcular el promedio, lo que est√°s estimando es la ganancia esperada por mano bajo esta pol√≠tica. Que el promedio salga negativo y relativamente lejos de 0 (‚àí0.3836) indica que pierdes m√°s manos de las que ganas, y que los empates no alcanzan a compensar esas p√©rdidas. La desviaci√≥n est√°ndar de aproximadamente 0.90 refleja que los resultados individuales de cada mano var√≠an harto (a veces ganas, a veces pierdes, a veces empatas), pero cuando promedias muchas manos, esa variabilidad se ‚Äúpromedia‚Äù alrededor de un valor claramente negativo.\n"
      ],
      "metadata": {
        "id": "skUeX4eWh7FF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEO_dY4x_SJu"
      },
      "source": [
        "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m9JsFA1wGmnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "129edf50-1fd6-424f-8052-3f5af62842e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.33     |\n",
            "|    ep_rew_mean     | -0.37    |\n",
            "| time/              |          |\n",
            "|    fps             | 887      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.44        |\n",
            "|    ep_rew_mean          | -0.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 681         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019450834 |\n",
            "|    clip_fraction        | 0.339       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.675      |\n",
            "|    explained_variance   | -0.0386     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.291       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.052      |\n",
            "|    value_loss           | 0.738       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.23        |\n",
            "|    ep_rew_mean          | -0.29       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 633         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018895764 |\n",
            "|    clip_fraction        | 0.285       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.626      |\n",
            "|    explained_variance   | 0.0635      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.342       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0447     |\n",
            "|    value_loss           | 0.746       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.28        |\n",
            "|    ep_rew_mean          | -0.24       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 572         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032271814 |\n",
            "|    clip_fraction        | 0.246       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.558      |\n",
            "|    explained_variance   | 0.147       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.421       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0388     |\n",
            "|    value_loss           | 0.734       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.3         |\n",
            "|    ep_rew_mean          | -0.22       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 568         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016463012 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.5        |\n",
            "|    explained_variance   | 0.156       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.375       |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0189     |\n",
            "|    value_loss           | 0.742       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.34        |\n",
            "|    ep_rew_mean          | -0.17       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 565         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 21          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008616139 |\n",
            "|    clip_fraction        | 0.0757      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.456      |\n",
            "|    explained_variance   | 0.183       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.326       |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0115     |\n",
            "|    value_loss           | 0.711       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.26        |\n",
            "|    ep_rew_mean          | 0.06        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 545         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009292877 |\n",
            "|    clip_fraction        | 0.104       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.417      |\n",
            "|    explained_variance   | 0.201       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.334       |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    value_loss           | 0.698       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.3         |\n",
            "|    ep_rew_mean          | -0.09       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 547         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 29          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007817911 |\n",
            "|    clip_fraction        | 0.073       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.386      |\n",
            "|    explained_variance   | 0.205       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.333       |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.00969    |\n",
            "|    value_loss           | 0.689       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.32         |\n",
            "|    ep_rew_mean          | -0.09        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 549          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047155614 |\n",
            "|    clip_fraction        | 0.0486       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.367       |\n",
            "|    explained_variance   | 0.204        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.278        |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00545     |\n",
            "|    value_loss           | 0.696        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.34        |\n",
            "|    ep_rew_mean          | -0.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 541         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 37          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007115972 |\n",
            "|    clip_fraction        | 0.0799      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.344      |\n",
            "|    explained_variance   | 0.185       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.328       |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00875    |\n",
            "|    value_loss           | 0.712       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.36        |\n",
            "|    ep_rew_mean          | -0.08       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 537         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 41          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007944451 |\n",
            "|    clip_fraction        | 0.0676      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.315      |\n",
            "|    explained_variance   | 0.18        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.391       |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00897    |\n",
            "|    value_loss           | 0.724       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.38         |\n",
            "|    ep_rew_mean          | -0.08        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 538          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 45           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054489495 |\n",
            "|    clip_fraction        | 0.0507       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.289       |\n",
            "|    explained_variance   | 0.207        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.317        |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00614     |\n",
            "|    value_loss           | 0.69         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.21        |\n",
            "|    ep_rew_mean          | -0.06       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 535         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004400594 |\n",
            "|    clip_fraction        | 0.0461      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.262      |\n",
            "|    explained_variance   | 0.206       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.288       |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.00557    |\n",
            "|    value_loss           | 0.698       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.46         |\n",
            "|    ep_rew_mean          | -0.09        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 524          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 54           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039019717 |\n",
            "|    clip_fraction        | 0.0375       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.251       |\n",
            "|    explained_variance   | 0.198        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.311        |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.00418     |\n",
            "|    value_loss           | 0.692        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.47         |\n",
            "|    ep_rew_mean          | -0.08        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 523          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 58           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045702066 |\n",
            "|    clip_fraction        | 0.0439       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.242       |\n",
            "|    explained_variance   | 0.198        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.388        |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00578     |\n",
            "|    value_loss           | 0.702        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.47        |\n",
            "|    ep_rew_mean          | -0.06       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 521         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 62          |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005150982 |\n",
            "|    clip_fraction        | 0.04        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.231      |\n",
            "|    explained_variance   | 0.21        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.309       |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.00438    |\n",
            "|    value_loss           | 0.684       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.4          |\n",
            "|    ep_rew_mean          | 0.1          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 520          |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 66           |\n",
            "|    total_timesteps      | 34816        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027849632 |\n",
            "|    clip_fraction        | 0.034        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.225       |\n",
            "|    explained_variance   | 0.232        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.318        |\n",
            "|    n_updates            | 160          |\n",
            "|    policy_gradient_loss | -0.00243     |\n",
            "|    value_loss           | 0.664        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.41         |\n",
            "|    ep_rew_mean          | -0.07        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 522          |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 70           |\n",
            "|    total_timesteps      | 36864        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054525067 |\n",
            "|    clip_fraction        | 0.0415       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.191       |\n",
            "|    explained_variance   | 0.223        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.237        |\n",
            "|    n_updates            | 170          |\n",
            "|    policy_gradient_loss | -0.00526     |\n",
            "|    value_loss           | 0.671        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.43         |\n",
            "|    ep_rew_mean          | 0.02         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 523          |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 74           |\n",
            "|    total_timesteps      | 38912        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038874922 |\n",
            "|    clip_fraction        | 0.0401       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.186       |\n",
            "|    explained_variance   | 0.224        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.343        |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.00446     |\n",
            "|    value_loss           | 0.679        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.54        |\n",
            "|    ep_rew_mean          | -0.04       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 519         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003889805 |\n",
            "|    clip_fraction        | 0.0351      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.166      |\n",
            "|    explained_variance   | 0.24        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.301       |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.00365    |\n",
            "|    value_loss           | 0.665       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.46         |\n",
            "|    ep_rew_mean          | -0.06        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 520          |\n",
            "|    iterations           | 21           |\n",
            "|    time_elapsed         | 82           |\n",
            "|    total_timesteps      | 43008        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034783888 |\n",
            "|    clip_fraction        | 0.0381       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.148       |\n",
            "|    explained_variance   | 0.219        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.38         |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.00388     |\n",
            "|    value_loss           | 0.665        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.34        |\n",
            "|    ep_rew_mean          | 0.07        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 522         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 86          |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003374607 |\n",
            "|    clip_fraction        | 0.0346      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.137      |\n",
            "|    explained_variance   | 0.198       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.355       |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.00468    |\n",
            "|    value_loss           | 0.686       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.44        |\n",
            "|    ep_rew_mean          | -0.33       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 519         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 90          |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004826146 |\n",
            "|    clip_fraction        | 0.0386      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.14       |\n",
            "|    explained_variance   | 0.229       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.417       |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.00369    |\n",
            "|    value_loss           | 0.661       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.43        |\n",
            "|    ep_rew_mean          | -0.06       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 521         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 94          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004280748 |\n",
            "|    clip_fraction        | 0.0315      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.142      |\n",
            "|    explained_variance   | 0.193       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.401       |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.00314    |\n",
            "|    value_loss           | 0.7         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.36        |\n",
            "|    ep_rew_mean          | -0.09       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 523         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 97          |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002561071 |\n",
            "|    clip_fraction        | 0.0257      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.133      |\n",
            "|    explained_variance   | 0.237       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.286       |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.00243    |\n",
            "|    value_loss           | 0.663       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.54         |\n",
            "|    ep_rew_mean          | -0.02        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 521          |\n",
            "|    iterations           | 26           |\n",
            "|    time_elapsed         | 102          |\n",
            "|    total_timesteps      | 53248        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035685971 |\n",
            "|    clip_fraction        | 0.0309       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.128       |\n",
            "|    explained_variance   | 0.225        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.385        |\n",
            "|    n_updates            | 250          |\n",
            "|    policy_gradient_loss | -0.00334     |\n",
            "|    value_loss           | 0.668        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.57         |\n",
            "|    ep_rew_mean          | 0.07         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 521          |\n",
            "|    iterations           | 27           |\n",
            "|    time_elapsed         | 106          |\n",
            "|    total_timesteps      | 55296        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025166662 |\n",
            "|    clip_fraction        | 0.0294       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.126       |\n",
            "|    explained_variance   | 0.213        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.336        |\n",
            "|    n_updates            | 260          |\n",
            "|    policy_gradient_loss | -0.00186     |\n",
            "|    value_loss           | 0.658        |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.44       |\n",
            "|    ep_rew_mean          | -0.09      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 523        |\n",
            "|    iterations           | 28         |\n",
            "|    time_elapsed         | 109        |\n",
            "|    total_timesteps      | 57344      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00319895 |\n",
            "|    clip_fraction        | 0.0227     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.113     |\n",
            "|    explained_variance   | 0.193      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.354      |\n",
            "|    n_updates            | 270        |\n",
            "|    policy_gradient_loss | -0.00206   |\n",
            "|    value_loss           | 0.698      |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.35         |\n",
            "|    ep_rew_mean          | -0.21        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 523          |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 113          |\n",
            "|    total_timesteps      | 59392        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019146826 |\n",
            "|    clip_fraction        | 0.0216       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.115       |\n",
            "|    explained_variance   | 0.202        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.378        |\n",
            "|    n_updates            | 280          |\n",
            "|    policy_gradient_loss | -0.00114     |\n",
            "|    value_loss           | 0.695        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.42         |\n",
            "|    ep_rew_mean          | -0.23        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 522          |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 117          |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022058901 |\n",
            "|    clip_fraction        | 0.0244       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.105       |\n",
            "|    explained_variance   | 0.191        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.285        |\n",
            "|    n_updates            | 290          |\n",
            "|    policy_gradient_loss | -0.0011      |\n",
            "|    value_loss           | 0.69         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.35         |\n",
            "|    ep_rew_mean          | -0.15        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 523          |\n",
            "|    iterations           | 31           |\n",
            "|    time_elapsed         | 121          |\n",
            "|    total_timesteps      | 63488        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039176065 |\n",
            "|    clip_fraction        | 0.0257       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.104       |\n",
            "|    explained_variance   | 0.204        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.374        |\n",
            "|    n_updates            | 300          |\n",
            "|    policy_gradient_loss | -0.00213     |\n",
            "|    value_loss           | 0.688        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.4          |\n",
            "|    ep_rew_mean          | -0.16        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 525          |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 124          |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019700709 |\n",
            "|    clip_fraction        | 0.0138       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.106       |\n",
            "|    explained_variance   | 0.183        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.387        |\n",
            "|    n_updates            | 310          |\n",
            "|    policy_gradient_loss | -0.000549    |\n",
            "|    value_loss           | 0.714        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.42        |\n",
            "|    ep_rew_mean          | -0.04       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 522         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 129         |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002933758 |\n",
            "|    clip_fraction        | 0.023       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.11       |\n",
            "|    explained_variance   | 0.186       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.431       |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.00196    |\n",
            "|    value_loss           | 0.684       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.45         |\n",
            "|    ep_rew_mean          | -0.01        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 523          |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 132          |\n",
            "|    total_timesteps      | 69632        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021439798 |\n",
            "|    clip_fraction        | 0.0255       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.1         |\n",
            "|    explained_variance   | 0.245        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.312        |\n",
            "|    n_updates            | 330          |\n",
            "|    policy_gradient_loss | -0.00169     |\n",
            "|    value_loss           | 0.657        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.41        |\n",
            "|    ep_rew_mean          | -0.08       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 525         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 136         |\n",
            "|    total_timesteps      | 71680       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004110389 |\n",
            "|    clip_fraction        | 0.025       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0937     |\n",
            "|    explained_variance   | 0.227       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.34        |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | -0.00198    |\n",
            "|    value_loss           | 0.664       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.53         |\n",
            "|    ep_rew_mean          | -0.18        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 524          |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 140          |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043070894 |\n",
            "|    clip_fraction        | 0.0223       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0959      |\n",
            "|    explained_variance   | 0.178        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.358        |\n",
            "|    n_updates            | 350          |\n",
            "|    policy_gradient_loss | -0.00139     |\n",
            "|    value_loss           | 0.679        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.41        |\n",
            "|    ep_rew_mean          | 0.02        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 524         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 144         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005266117 |\n",
            "|    clip_fraction        | 0.0334      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.103      |\n",
            "|    explained_variance   | 0.224       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.28        |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.00298    |\n",
            "|    value_loss           | 0.664       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.45         |\n",
            "|    ep_rew_mean          | -0.06        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 525          |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 148          |\n",
            "|    total_timesteps      | 77824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029286637 |\n",
            "|    clip_fraction        | 0.0288       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0998      |\n",
            "|    explained_variance   | 0.203        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.349        |\n",
            "|    n_updates            | 370          |\n",
            "|    policy_gradient_loss | -0.00159     |\n",
            "|    value_loss           | 0.674        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.38         |\n",
            "|    ep_rew_mean          | -0.07        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 516          |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 154          |\n",
            "|    total_timesteps      | 79872        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034722462 |\n",
            "|    clip_fraction        | 0.031        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0928      |\n",
            "|    explained_variance   | 0.246        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.296        |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | -0.00279     |\n",
            "|    value_loss           | 0.644        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.41         |\n",
            "|    ep_rew_mean          | -0.03        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 517          |\n",
            "|    iterations           | 40           |\n",
            "|    time_elapsed         | 158          |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027300292 |\n",
            "|    clip_fraction        | 0.0365       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0896      |\n",
            "|    explained_variance   | 0.205        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.376        |\n",
            "|    n_updates            | 390          |\n",
            "|    policy_gradient_loss | -0.00204     |\n",
            "|    value_loss           | 0.679        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.45        |\n",
            "|    ep_rew_mean          | -0.01       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 518         |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 161         |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004155948 |\n",
            "|    clip_fraction        | 0.0291      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0835     |\n",
            "|    explained_variance   | 0.21        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.278       |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.00311    |\n",
            "|    value_loss           | 0.677       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.45         |\n",
            "|    ep_rew_mean          | -0.06        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 518          |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 165          |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047277124 |\n",
            "|    clip_fraction        | 0.0222       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0766      |\n",
            "|    explained_variance   | 0.187        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.304        |\n",
            "|    n_updates            | 410          |\n",
            "|    policy_gradient_loss | -0.002       |\n",
            "|    value_loss           | 0.694        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.45        |\n",
            "|    ep_rew_mean          | -0.04       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 517         |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 170         |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006969616 |\n",
            "|    clip_fraction        | 0.0256      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.078      |\n",
            "|    explained_variance   | 0.199       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.299       |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.00298    |\n",
            "|    value_loss           | 0.683       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.4         |\n",
            "|    ep_rew_mean          | 0.17        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 518         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 173         |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002420933 |\n",
            "|    clip_fraction        | 0.0261      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0738     |\n",
            "|    explained_variance   | 0.202       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.334       |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.00238    |\n",
            "|    value_loss           | 0.675       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.54        |\n",
            "|    ep_rew_mean          | -0.11       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 519         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 177         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001735122 |\n",
            "|    clip_fraction        | 0.0199      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.071      |\n",
            "|    explained_variance   | 0.2         |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.382       |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.000808   |\n",
            "|    value_loss           | 0.684       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.5          |\n",
            "|    ep_rew_mean          | -0.03        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 518          |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 181          |\n",
            "|    total_timesteps      | 94208        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029692687 |\n",
            "|    clip_fraction        | 0.0222       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0743      |\n",
            "|    explained_variance   | 0.205        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.33         |\n",
            "|    n_updates            | 450          |\n",
            "|    policy_gradient_loss | -0.0016      |\n",
            "|    value_loss           | 0.679        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.45         |\n",
            "|    ep_rew_mean          | -0.06        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 519          |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 185          |\n",
            "|    total_timesteps      | 96256        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031438954 |\n",
            "|    clip_fraction        | 0.0156       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0607      |\n",
            "|    explained_variance   | 0.224        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.22         |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | -0.000841    |\n",
            "|    value_loss           | 0.671        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.39        |\n",
            "|    ep_rew_mean          | -0.07       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 520         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 189         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003569459 |\n",
            "|    clip_fraction        | 0.0125      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0578     |\n",
            "|    explained_variance   | 0.194       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.356       |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.00197    |\n",
            "|    value_loss           | 0.697       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.44         |\n",
            "|    ep_rew_mean          | -0.19        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 518          |\n",
            "|    iterations           | 49           |\n",
            "|    time_elapsed         | 193          |\n",
            "|    total_timesteps      | 100352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044713737 |\n",
            "|    clip_fraction        | 0.0196       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0463      |\n",
            "|    explained_variance   | 0.199        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.345        |\n",
            "|    n_updates            | 480          |\n",
            "|    policy_gradient_loss | -0.00217     |\n",
            "|    value_loss           | 0.686        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.57        |\n",
            "|    ep_rew_mean          | -0.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 517         |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 197         |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001670276 |\n",
            "|    clip_fraction        | 0.0125      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0423     |\n",
            "|    explained_variance   | 0.214       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.32        |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | -0.000791   |\n",
            "|    value_loss           | 0.668       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.4         |\n",
            "|    ep_rew_mean          | -0.13       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 516         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 202         |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003959533 |\n",
            "|    clip_fraction        | 0.0162      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0409     |\n",
            "|    explained_variance   | 0.218       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.276       |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.00176    |\n",
            "|    value_loss           | 0.684       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.46         |\n",
            "|    ep_rew_mean          | 0.11         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 515          |\n",
            "|    iterations           | 52           |\n",
            "|    time_elapsed         | 206          |\n",
            "|    total_timesteps      | 106496       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032087343 |\n",
            "|    clip_fraction        | 0.0236       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0479      |\n",
            "|    explained_variance   | 0.184        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.329        |\n",
            "|    n_updates            | 510          |\n",
            "|    policy_gradient_loss | -0.00351     |\n",
            "|    value_loss           | 0.689        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.62         |\n",
            "|    ep_rew_mean          | -0.18        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 516          |\n",
            "|    iterations           | 53           |\n",
            "|    time_elapsed         | 210          |\n",
            "|    total_timesteps      | 108544       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030340077 |\n",
            "|    clip_fraction        | 0.0244       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0572      |\n",
            "|    explained_variance   | 0.192        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.303        |\n",
            "|    n_updates            | 520          |\n",
            "|    policy_gradient_loss | -0.00158     |\n",
            "|    value_loss           | 0.698        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.52         |\n",
            "|    ep_rew_mean          | -0.03        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 517          |\n",
            "|    iterations           | 54           |\n",
            "|    time_elapsed         | 213          |\n",
            "|    total_timesteps      | 110592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031427382 |\n",
            "|    clip_fraction        | 0.0299       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.06        |\n",
            "|    explained_variance   | 0.2          |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.302        |\n",
            "|    n_updates            | 530          |\n",
            "|    policy_gradient_loss | -0.00201     |\n",
            "|    value_loss           | 0.682        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.49         |\n",
            "|    ep_rew_mean          | -0.22        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 516          |\n",
            "|    iterations           | 55           |\n",
            "|    time_elapsed         | 217          |\n",
            "|    total_timesteps      | 112640       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047017075 |\n",
            "|    clip_fraction        | 0.0311       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0669      |\n",
            "|    explained_variance   | 0.224        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.397        |\n",
            "|    n_updates            | 540          |\n",
            "|    policy_gradient_loss | -0.000774    |\n",
            "|    value_loss           | 0.66         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.48         |\n",
            "|    ep_rew_mean          | 0.19         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 516          |\n",
            "|    iterations           | 56           |\n",
            "|    time_elapsed         | 221          |\n",
            "|    total_timesteps      | 114688       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029777419 |\n",
            "|    clip_fraction        | 0.0255       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.065       |\n",
            "|    explained_variance   | 0.229        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.304        |\n",
            "|    n_updates            | 550          |\n",
            "|    policy_gradient_loss | -0.00154     |\n",
            "|    value_loss           | 0.654        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.44        |\n",
            "|    ep_rew_mean          | -0.04       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 517         |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 225         |\n",
            "|    total_timesteps      | 116736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004942115 |\n",
            "|    clip_fraction        | 0.0328      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0704     |\n",
            "|    explained_variance   | 0.196       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.259       |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.00288    |\n",
            "|    value_loss           | 0.676       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.45         |\n",
            "|    ep_rew_mean          | 0.08         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 518          |\n",
            "|    iterations           | 58           |\n",
            "|    time_elapsed         | 229          |\n",
            "|    total_timesteps      | 118784       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026771135 |\n",
            "|    clip_fraction        | 0.0262       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.081       |\n",
            "|    explained_variance   | 0.216        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.294        |\n",
            "|    n_updates            | 570          |\n",
            "|    policy_gradient_loss | -0.000247    |\n",
            "|    value_loss           | 0.666        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.48         |\n",
            "|    ep_rew_mean          | -0.09        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 517          |\n",
            "|    iterations           | 59           |\n",
            "|    time_elapsed         | 233          |\n",
            "|    total_timesteps      | 120832       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047243726 |\n",
            "|    clip_fraction        | 0.0282       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0765      |\n",
            "|    explained_variance   | 0.194        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.335        |\n",
            "|    n_updates            | 580          |\n",
            "|    policy_gradient_loss | -0.00191     |\n",
            "|    value_loss           | 0.69         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.41         |\n",
            "|    ep_rew_mean          | -0.16        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 518          |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 237          |\n",
            "|    total_timesteps      | 122880       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040105935 |\n",
            "|    clip_fraction        | 0.033        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0698      |\n",
            "|    explained_variance   | 0.226        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.292        |\n",
            "|    n_updates            | 590          |\n",
            "|    policy_gradient_loss | -0.00305     |\n",
            "|    value_loss           | 0.66         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.46         |\n",
            "|    ep_rew_mean          | -0.11        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 518          |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 240          |\n",
            "|    total_timesteps      | 124928       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036734873 |\n",
            "|    clip_fraction        | 0.0289       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0794      |\n",
            "|    explained_variance   | 0.215        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.316        |\n",
            "|    n_updates            | 600          |\n",
            "|    policy_gradient_loss | -0.000995    |\n",
            "|    value_loss           | 0.674        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.6          |\n",
            "|    ep_rew_mean          | 0.11         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 517          |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 245          |\n",
            "|    total_timesteps      | 126976       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034230775 |\n",
            "|    clip_fraction        | 0.0303       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0701      |\n",
            "|    explained_variance   | 0.166        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.324        |\n",
            "|    n_updates            | 610          |\n",
            "|    policy_gradient_loss | -0.00221     |\n",
            "|    value_loss           | 0.703        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.53         |\n",
            "|    ep_rew_mean          | -0.13        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 518          |\n",
            "|    iterations           | 63           |\n",
            "|    time_elapsed         | 248          |\n",
            "|    total_timesteps      | 129024       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046671806 |\n",
            "|    clip_fraction        | 0.0288       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0827      |\n",
            "|    explained_variance   | 0.206        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.372        |\n",
            "|    n_updates            | 620          |\n",
            "|    policy_gradient_loss | -0.00292     |\n",
            "|    value_loss           | 0.695        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.51        |\n",
            "|    ep_rew_mean          | -0.18       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 519         |\n",
            "|    iterations           | 64          |\n",
            "|    time_elapsed         | 252         |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004472251 |\n",
            "|    clip_fraction        | 0.0356      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0757     |\n",
            "|    explained_variance   | 0.207       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.352       |\n",
            "|    n_updates            | 630         |\n",
            "|    policy_gradient_loss | -0.00277    |\n",
            "|    value_loss           | 0.668       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.37         |\n",
            "|    ep_rew_mean          | -0.11        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 518          |\n",
            "|    iterations           | 65           |\n",
            "|    time_elapsed         | 256          |\n",
            "|    total_timesteps      | 133120       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038153771 |\n",
            "|    clip_fraction        | 0.028        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0772      |\n",
            "|    explained_variance   | 0.211        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.371        |\n",
            "|    n_updates            | 640          |\n",
            "|    policy_gradient_loss | -0.000875    |\n",
            "|    value_loss           | 0.677        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.63         |\n",
            "|    ep_rew_mean          | 0            |\n",
            "| time/                   |              |\n",
            "|    fps                  | 518          |\n",
            "|    iterations           | 66           |\n",
            "|    time_elapsed         | 260          |\n",
            "|    total_timesteps      | 135168       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036278742 |\n",
            "|    clip_fraction        | 0.0222       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0711      |\n",
            "|    explained_variance   | 0.237        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.349        |\n",
            "|    n_updates            | 650          |\n",
            "|    policy_gradient_loss | -0.00049     |\n",
            "|    value_loss           | 0.654        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.58         |\n",
            "|    ep_rew_mean          | -0.11        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 519          |\n",
            "|    iterations           | 67           |\n",
            "|    time_elapsed         | 264          |\n",
            "|    total_timesteps      | 137216       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028218636 |\n",
            "|    clip_fraction        | 0.0217       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0739      |\n",
            "|    explained_variance   | 0.216        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.353        |\n",
            "|    n_updates            | 660          |\n",
            "|    policy_gradient_loss | -0.0019      |\n",
            "|    value_loss           | 0.663        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.47         |\n",
            "|    ep_rew_mean          | 0            |\n",
            "| time/                   |              |\n",
            "|    fps                  | 519          |\n",
            "|    iterations           | 68           |\n",
            "|    time_elapsed         | 267          |\n",
            "|    total_timesteps      | 139264       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027988143 |\n",
            "|    clip_fraction        | 0.0203       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0693      |\n",
            "|    explained_variance   | 0.22         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.353        |\n",
            "|    n_updates            | 670          |\n",
            "|    policy_gradient_loss | -0.00083     |\n",
            "|    value_loss           | 0.676        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.48        |\n",
            "|    ep_rew_mean          | -0.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 518         |\n",
            "|    iterations           | 69          |\n",
            "|    time_elapsed         | 272         |\n",
            "|    total_timesteps      | 141312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001985695 |\n",
            "|    clip_fraction        | 0.0217      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0771     |\n",
            "|    explained_variance   | 0.201       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.342       |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | -0.000303   |\n",
            "|    value_loss           | 0.683       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.4         |\n",
            "|    ep_rew_mean          | -0.08       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 519         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 275         |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002080707 |\n",
            "|    clip_fraction        | 0.02        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0609     |\n",
            "|    explained_variance   | 0.207       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.383       |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | -0.000752   |\n",
            "|    value_loss           | 0.667       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.53         |\n",
            "|    ep_rew_mean          | 0.03         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 520          |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 279          |\n",
            "|    total_timesteps      | 145408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035643093 |\n",
            "|    clip_fraction        | 0.0166       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.058       |\n",
            "|    explained_variance   | 0.209        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.353        |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | -0.000126    |\n",
            "|    value_loss           | 0.682        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.46         |\n",
            "|    ep_rew_mean          | -0.04        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 519          |\n",
            "|    iterations           | 72           |\n",
            "|    time_elapsed         | 283          |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022627125 |\n",
            "|    clip_fraction        | 0.018        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0583      |\n",
            "|    explained_variance   | 0.198        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.394        |\n",
            "|    n_updates            | 710          |\n",
            "|    policy_gradient_loss | -7.3e-05     |\n",
            "|    value_loss           | 0.689        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.42        |\n",
            "|    ep_rew_mean          | 0.08        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 517         |\n",
            "|    iterations           | 73          |\n",
            "|    time_elapsed         | 289         |\n",
            "|    total_timesteps      | 149504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002131685 |\n",
            "|    clip_fraction        | 0.0179      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0668     |\n",
            "|    explained_variance   | 0.226       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.319       |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | -0.00063    |\n",
            "|    value_loss           | 0.665       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.57         |\n",
            "|    ep_rew_mean          | -0.02        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 514          |\n",
            "|    iterations           | 74           |\n",
            "|    time_elapsed         | 294          |\n",
            "|    total_timesteps      | 151552       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031664413 |\n",
            "|    clip_fraction        | 0.0225       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0699      |\n",
            "|    explained_variance   | 0.185        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.396        |\n",
            "|    n_updates            | 730          |\n",
            "|    policy_gradient_loss | -0.00242     |\n",
            "|    value_loss           | 0.689        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.47        |\n",
            "|    ep_rew_mean          | -0.14       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 514         |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 298         |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005950805 |\n",
            "|    clip_fraction        | 0.0204      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.063      |\n",
            "|    explained_variance   | 0.206       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.377       |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | -0.000504   |\n",
            "|    value_loss           | 0.672       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.52        |\n",
            "|    ep_rew_mean          | 0.05        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 515         |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 302         |\n",
            "|    total_timesteps      | 155648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004953011 |\n",
            "|    clip_fraction        | 0.0175      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0671     |\n",
            "|    explained_variance   | 0.221       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.296       |\n",
            "|    n_updates            | 750         |\n",
            "|    policy_gradient_loss | -0.000911   |\n",
            "|    value_loss           | 0.654       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.56         |\n",
            "|    ep_rew_mean          | 0.04         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 515          |\n",
            "|    iterations           | 77           |\n",
            "|    time_elapsed         | 305          |\n",
            "|    total_timesteps      | 157696       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0075361705 |\n",
            "|    clip_fraction        | 0.0292       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0611      |\n",
            "|    explained_variance   | 0.206        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.36         |\n",
            "|    n_updates            | 760          |\n",
            "|    policy_gradient_loss | -0.00326     |\n",
            "|    value_loss           | 0.672        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.42         |\n",
            "|    ep_rew_mean          | 0.02         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 514          |\n",
            "|    iterations           | 78           |\n",
            "|    time_elapsed         | 310          |\n",
            "|    total_timesteps      | 159744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025180113 |\n",
            "|    clip_fraction        | 0.0222       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0714      |\n",
            "|    explained_variance   | 0.207        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.323        |\n",
            "|    n_updates            | 770          |\n",
            "|    policy_gradient_loss | -5.84e-05    |\n",
            "|    value_loss           | 0.671        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.47        |\n",
            "|    ep_rew_mean          | -0.15       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 515         |\n",
            "|    iterations           | 79          |\n",
            "|    time_elapsed         | 313         |\n",
            "|    total_timesteps      | 161792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005795325 |\n",
            "|    clip_fraction        | 0.0298      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0654     |\n",
            "|    explained_variance   | 0.192       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.344       |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | -0.00272    |\n",
            "|    value_loss           | 0.69        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.42         |\n",
            "|    ep_rew_mean          | 0            |\n",
            "| time/                   |              |\n",
            "|    fps                  | 516          |\n",
            "|    iterations           | 80           |\n",
            "|    time_elapsed         | 317          |\n",
            "|    total_timesteps      | 163840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043562083 |\n",
            "|    clip_fraction        | 0.0216       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0636      |\n",
            "|    explained_variance   | 0.223        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.388        |\n",
            "|    n_updates            | 790          |\n",
            "|    policy_gradient_loss | -0.00245     |\n",
            "|    value_loss           | 0.657        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.59         |\n",
            "|    ep_rew_mean          | -0.11        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 515          |\n",
            "|    iterations           | 81           |\n",
            "|    time_elapsed         | 321          |\n",
            "|    total_timesteps      | 165888       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037025353 |\n",
            "|    clip_fraction        | 0.0267       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0605      |\n",
            "|    explained_variance   | 0.191        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.368        |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | -0.00159     |\n",
            "|    value_loss           | 0.704        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.46         |\n",
            "|    ep_rew_mean          | 0.01         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 515          |\n",
            "|    iterations           | 82           |\n",
            "|    time_elapsed         | 325          |\n",
            "|    total_timesteps      | 167936       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036860264 |\n",
            "|    clip_fraction        | 0.0241       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0619      |\n",
            "|    explained_variance   | 0.224        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.327        |\n",
            "|    n_updates            | 810          |\n",
            "|    policy_gradient_loss | -0.00118     |\n",
            "|    value_loss           | 0.683        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.49        |\n",
            "|    ep_rew_mean          | 0.01        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 516         |\n",
            "|    iterations           | 83          |\n",
            "|    time_elapsed         | 329         |\n",
            "|    total_timesteps      | 169984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002945161 |\n",
            "|    clip_fraction        | 0.0207      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0616     |\n",
            "|    explained_variance   | 0.207       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.381       |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | -0.00123    |\n",
            "|    value_loss           | 0.678       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.57         |\n",
            "|    ep_rew_mean          | 0.06         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 516          |\n",
            "|    iterations           | 84           |\n",
            "|    time_elapsed         | 333          |\n",
            "|    total_timesteps      | 172032       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0100357905 |\n",
            "|    clip_fraction        | 0.029        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.057       |\n",
            "|    explained_variance   | 0.194        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.331        |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | -0.00239     |\n",
            "|    value_loss           | 0.681        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.33         |\n",
            "|    ep_rew_mean          | 0.1          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 516          |\n",
            "|    iterations           | 85           |\n",
            "|    time_elapsed         | 337          |\n",
            "|    total_timesteps      | 174080       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035756489 |\n",
            "|    clip_fraction        | 0.0187       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0501      |\n",
            "|    explained_variance   | 0.205        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.362        |\n",
            "|    n_updates            | 840          |\n",
            "|    policy_gradient_loss | -0.00061     |\n",
            "|    value_loss           | 0.687        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.46         |\n",
            "|    ep_rew_mean          | -0.02        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 516          |\n",
            "|    iterations           | 86           |\n",
            "|    time_elapsed         | 340          |\n",
            "|    total_timesteps      | 176128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032209111 |\n",
            "|    clip_fraction        | 0.0166       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0515      |\n",
            "|    explained_variance   | 0.211        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.405        |\n",
            "|    n_updates            | 850          |\n",
            "|    policy_gradient_loss | -0.0017      |\n",
            "|    value_loss           | 0.659        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.55         |\n",
            "|    ep_rew_mean          | -0.09        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 517          |\n",
            "|    iterations           | 87           |\n",
            "|    time_elapsed         | 344          |\n",
            "|    total_timesteps      | 178176       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048334226 |\n",
            "|    clip_fraction        | 0.022        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0515      |\n",
            "|    explained_variance   | 0.189        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.41         |\n",
            "|    n_updates            | 860          |\n",
            "|    policy_gradient_loss | -0.0021      |\n",
            "|    value_loss           | 0.708        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.58         |\n",
            "|    ep_rew_mean          | -0.08        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 516          |\n",
            "|    iterations           | 88           |\n",
            "|    time_elapsed         | 348          |\n",
            "|    total_timesteps      | 180224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036459751 |\n",
            "|    clip_fraction        | 0.0216       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0613      |\n",
            "|    explained_variance   | 0.235        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.324        |\n",
            "|    n_updates            | 870          |\n",
            "|    policy_gradient_loss | 0.000268     |\n",
            "|    value_loss           | 0.664        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.57         |\n",
            "|    ep_rew_mean          | -0.22        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 516          |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 352          |\n",
            "|    total_timesteps      | 182272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031662583 |\n",
            "|    clip_fraction        | 0.0245       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0593      |\n",
            "|    explained_variance   | 0.207        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.352        |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -0.000424    |\n",
            "|    value_loss           | 0.688        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.36         |\n",
            "|    ep_rew_mean          | 0.04         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 517          |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 356          |\n",
            "|    total_timesteps      | 184320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022228938 |\n",
            "|    clip_fraction        | 0.0199       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0566      |\n",
            "|    explained_variance   | 0.2          |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.391        |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | -0.000455    |\n",
            "|    value_loss           | 0.686        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.56         |\n",
            "|    ep_rew_mean          | 0.18         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 516          |\n",
            "|    iterations           | 91           |\n",
            "|    time_elapsed         | 360          |\n",
            "|    total_timesteps      | 186368       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024306073 |\n",
            "|    clip_fraction        | 0.0196       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0592      |\n",
            "|    explained_variance   | 0.161        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.436        |\n",
            "|    n_updates            | 900          |\n",
            "|    policy_gradient_loss | 0.000227     |\n",
            "|    value_loss           | 0.712        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.46         |\n",
            "|    ep_rew_mean          | -0.12        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 517          |\n",
            "|    iterations           | 92           |\n",
            "|    time_elapsed         | 364          |\n",
            "|    total_timesteps      | 188416       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029924028 |\n",
            "|    clip_fraction        | 0.0223       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0636      |\n",
            "|    explained_variance   | 0.211        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.379        |\n",
            "|    n_updates            | 910          |\n",
            "|    policy_gradient_loss | -0.00146     |\n",
            "|    value_loss           | 0.681        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.49         |\n",
            "|    ep_rew_mean          | -0.15        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 517          |\n",
            "|    iterations           | 93           |\n",
            "|    time_elapsed         | 367          |\n",
            "|    total_timesteps      | 190464       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038804014 |\n",
            "|    clip_fraction        | 0.0291       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0572      |\n",
            "|    explained_variance   | 0.224        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.255        |\n",
            "|    n_updates            | 920          |\n",
            "|    policy_gradient_loss | -0.00155     |\n",
            "|    value_loss           | 0.668        |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.51       |\n",
            "|    ep_rew_mean          | -0.13      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 517        |\n",
            "|    iterations           | 94         |\n",
            "|    time_elapsed         | 371        |\n",
            "|    total_timesteps      | 192512     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00367541 |\n",
            "|    clip_fraction        | 0.0223     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.0536    |\n",
            "|    explained_variance   | 0.233      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.276      |\n",
            "|    n_updates            | 930        |\n",
            "|    policy_gradient_loss | -0.00149   |\n",
            "|    value_loss           | 0.662      |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.51         |\n",
            "|    ep_rew_mean          | -0.17        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 517          |\n",
            "|    iterations           | 95           |\n",
            "|    time_elapsed         | 376          |\n",
            "|    total_timesteps      | 194560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039328597 |\n",
            "|    clip_fraction        | 0.0288       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.062       |\n",
            "|    explained_variance   | 0.196        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.355        |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | -0.00149     |\n",
            "|    value_loss           | 0.677        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.49        |\n",
            "|    ep_rew_mean          | 0.02        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 517         |\n",
            "|    iterations           | 96          |\n",
            "|    time_elapsed         | 379         |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005484765 |\n",
            "|    clip_fraction        | 0.023       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0533     |\n",
            "|    explained_variance   | 0.193       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.28        |\n",
            "|    n_updates            | 950         |\n",
            "|    policy_gradient_loss | -0.00204    |\n",
            "|    value_loss           | 0.676       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.45        |\n",
            "|    ep_rew_mean          | -0.21       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 518         |\n",
            "|    iterations           | 97          |\n",
            "|    time_elapsed         | 383         |\n",
            "|    total_timesteps      | 198656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003965292 |\n",
            "|    clip_fraction        | 0.0157      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0522     |\n",
            "|    explained_variance   | 0.223       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.354       |\n",
            "|    n_updates            | 960         |\n",
            "|    policy_gradient_loss | -0.000577   |\n",
            "|    value_loss           | 0.683       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.61         |\n",
            "|    ep_rew_mean          | -0.14        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 517          |\n",
            "|    iterations           | 98           |\n",
            "|    time_elapsed         | 387          |\n",
            "|    total_timesteps      | 200704       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023672276 |\n",
            "|    clip_fraction        | 0.0216       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0584      |\n",
            "|    explained_variance   | 0.209        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.3          |\n",
            "|    n_updates            | 970          |\n",
            "|    policy_gradient_loss | -0.00135     |\n",
            "|    value_loss           | 0.683        |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7a35681383e0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)\n",
        "\n",
        "model = PPO(\n",
        "    policy=\"MlpPolicy\",\n",
        "    env=env,\n",
        "    verbose=1,\n",
        "    seed=0,\n",
        ")\n",
        "\n",
        "model.learn(total_timesteps=200_000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-bpdb8wZID1"
      },
      "source": [
        "#### **1.1.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
        "\n",
        "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5-d7d8GFf7F6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b34917-6182-411f-ab11-8f859815a734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas del agente: -0.069\n",
            "Desviaci√≥n est√°ndar: 0.9538548107547606\n"
          ]
        }
      ],
      "source": [
        "eval_env = gym.make(\"Blackjack-v1\")\n",
        "eval_env = FlattenObservation(eval_env)\n",
        "\n",
        "n_episodios = 5000\n",
        "recompensas = []\n",
        "\n",
        "for _ in range(n_episodios):\n",
        "    obs, info = eval_env.reset()\n",
        "    terminado = False\n",
        "    truncado = False\n",
        "    retorno = 0.0\n",
        "\n",
        "    while not (terminado or truncado):\n",
        "        accion, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminado, truncado, info = eval_env.step(accion)\n",
        "        retorno += reward\n",
        "\n",
        "    recompensas.append(retorno)\n",
        "\n",
        "recompensas = np.array(recompensas)\n",
        "promedio = recompensas.mean()\n",
        "desviacion = recompensas.std()\n",
        "\n",
        "print(\"Promedio de recompensas del agente:\", promedio)\n",
        "print(\"Desviaci√≥n est√°ndar:\", desviacion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El agente tiene un desempe√±o mejor que la pol√≠tica aleatoria. Con el baseline el promedio era -0.3836, es decir, se perd√≠a bastante en cada mano. Con el modelo PPO es de -0.0284, es decir, solo pierde alrededor de 0.028 unidades por mano, por lo que casi no pierde. Aunque todav√≠a la recompensa esperada es un poco negativa, el nuevo modelo reduce much√≠simo las p√©rdidas y juega mejor que el baseline aleatorio."
      ],
      "metadata": {
        "id": "Vr9H9Assi08z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO-EsAaPAYEm"
      },
      "source": [
        "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
        "\n",
        "Genere una funci√≥n que reciba un estado y retorne la accion del agente. Luego, use esta funci√≥n para entregar la acci√≥n escogida frente a los siguientes escenarios:\n",
        "\n",
        "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
        "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
        "\n",
        "¬øSon coherentes sus acciones con las reglas del juego?\n",
        "\n",
        "Hint: ¬øA que clase de python pertenecen los estados? Pruebe a usar el m√©todo `.reset` para saberlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh8XlGyzwtRp"
      },
      "outputs": [],
      "source": [
        "def accion_agente_desde_estado(model, player_sum, dealer_card, usable_ace):\n",
        "    estado = np.array([player_sum, dealer_card, int(usable_ace)], dtype=np.int32)\n",
        "\n",
        "    accion, _ = model.predict(estado, deterministic=True)\n",
        "    return int(accion)\n",
        "\n",
        "accion_1 = accion_agente_desde_estado(model, player_sum=6, dealer_card=7, usable_ace=False)\n",
        "print(\"Escenario 1 (6 vs 7, sin As): acci√≥n =\", accion_1)\n",
        "\n",
        "accion_2 = accion_agente_desde_estado(model, player_sum=19, dealer_card=3, usable_ace=True)\n",
        "print(\"Escenario 2 (19 vs 3, con As): acci√≥n =\", accion_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el primer escenario (suma 6 contra 7, sin As usable), la acci√≥n fue 1, es decir, pedir carta. Esto tiene sentido porque 6 es una mano muy d√©bil: no tiene sentido plantarse, lo razonable es seguir robando cartas para intentar acercarse a 21.\n",
        "\n",
        "En el segundo escenario (19 contra 3, con As usable), la acci√≥n fue 0, es decir, plantarse. Tener 19 es una mano muy fuerte, y el riesgo de pasarse si pides otra carta es alto. Adem√°s, el dealer con 3 tampoco tiene una posici√≥n especialmente fuerte. Por eso, plantarse es una decisi√≥n coherente con una estrategia de juego razonable."
      ],
      "metadata": {
        "id": "qOKdjfsklZTC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEqCTqqroh03"
      },
      "source": [
        "### **1.2 LunarLander**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la secci√≥n 2.1, en esta secci√≥n usted se encargar√° de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
        "\n",
        "Comencemos preparando el ambiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\", continuous=True)\n"
      ],
      "metadata": {
        "id": "dTOm9vn8m45H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBU4lGX3wpN6"
      },
      "source": [
        "Noten que se especifica el par√°metro `continuous = True`. ¬øQue implicancias tiene esto sobre el ambiente?\n",
        "\n",
        "Adem√°s, se le facilita la funci√≥n `export_gif` para el ejercicio 2.2.4:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poner continuous = True significa que las acciones del agente sean valores continuos en vez de elegir entre unas pocas opciones fijas. Eso hace que el espacio de acciones sea mucho m√°s grande y el problema m√°s dif√≠cil de aprender, pero con decisiones m√°s precisas en cada paso."
      ],
      "metadata": {
        "id": "JJY92aupnDRA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRiWpSo9yfr9"
      },
      "outputs": [],
      "source": [
        "def export_gif(model, n = 5):\n",
        "  '''\n",
        "  funci√≥n que exporta a gif el comportamiento del agente en n episodios\n",
        "  '''\n",
        "  images = []\n",
        "  for episode in range(n):\n",
        "    obs = model.env.reset()\n",
        "    img = model.env.render()\n",
        "    done = False\n",
        "    while not done:\n",
        "      images.append(img)\n",
        "      action, _ = model.predict(obs)\n",
        "      obs, reward, done, info = model.env.step(action)\n",
        "      img = model.env.render(mode=\"rgb_array\")\n",
        "\n",
        "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk5VJVppXh3N"
      },
      "source": [
        "#### **1.2.1 Descripci√≥n de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripci√≥n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¬øComo se distinguen las acciones de este ambiente en comparaci√≥n a `Blackjack`?\n",
        "\n",
        "Nota: recuerde que se especific√≥ el par√°metro `continuous = True`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb-u9LUE8O9a"
      },
      "source": [
        "El ambiente LunarLander se modela como un MDP donde el estado es un conjunto de n√∫meros reales que describen c√≥mo est√° el sistema en cada instante. La recompensa entrega valores positivos cuando el comportamiento va en la direcci√≥n deseada y valores negativos cuando el comportamiento es malo.\n",
        "\n",
        "Con continuous = True, las acciones tambi√©n son continuas: en cada paso el agente entrega uno o m√°s n√∫meros reales que representan la intensidad de los controles que aplica, dentro de un rango. En Blackjack, en cambio, las acciones son discretas (por ejemplo: pedir carta o quedarse). Es decir, en Blackjack el agente elige entre pocas opciones fijas, mientras que en LunarLander continuo el agente elige valores num√©ricos dentro de un espacio de acciones continuo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YChodtNQwzG2"
      },
      "source": [
        "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 10 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bwc3A0GX7a8"
      },
      "outputs": [],
      "source": [
        "n__episodes = 10\n",
        "recompensas = []\n",
        "\n",
        "for ep in range(n_episodes):\n",
        "    obs, info = env.reset()\n",
        "    terminado = False\n",
        "    recompensa_total = 0.0\n",
        "\n",
        "    while not terminado:\n",
        "        # Pol√≠tica aleatoria: acci√≥n al azar desde el espacio de acciones\n",
        "        action = env.action_space.sample()2\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        recompensa_total += reward\n",
        "        terminado = terminated or truncated\n",
        "\n",
        "    recompensas.append(recompensa_total)\n",
        "    print(f\"Episodio {ep+1}: recompensa total = {recompensa_total:.2f}\")\n",
        "\n",
        "recompensas = np.array(recompensas)\n",
        "print(f\"\\nPromedio de recompensas: {recompensas.mean():.4f}\")\n",
        "print(f\"Desviaci√≥n est√°ndar: {recompensas.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El desempe√±o de esta pol√≠tica aleatoria es claramente malo: el promedio de recompensa es fuertemente negativo, lo que muestra que en general el agente tiene episodios muy malos, y la desviaci√≥n est√°ndar alta indica que adem√°s los resultados son muy inestables entre episodios. En otras palabras, la pol√≠tica casi nunca logra un comportamiento razonable y solo sirve como una l√≠nea base muy simple para comparar despu√©s con pol√≠ticas entrenadas que deber√≠an obtener recompensas m√°s altas y menos negativas."
      ],
      "metadata": {
        "id": "yfQMCCZFpxk1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQrZVQflX_5f"
      },
      "source": [
        "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_6Ia9uoF7Hs"
      },
      "outputs": [],
      "source": [
        "model = PPO(\n",
        "    policy=\"MlpPolicy\",\n",
        "    env=env,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "model.learn(total_timesteps=10_000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z-oIUSrlAsY"
      },
      "source": [
        "#### **1.2.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
        "\n",
        "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ophyU3KrWrwl"
      },
      "outputs": [],
      "source": [
        "n_episodes = 10\n",
        "recompensas = []\n",
        "\n",
        "for ep in range(n_episodes):\n",
        "    obs, info = env.reset()\n",
        "    terminado = False\n",
        "    recompensa_total = 0.0\n",
        "\n",
        "    while not terminado:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        recompensa_total += reward\n",
        "        terminado = terminated or truncated\n",
        "\n",
        "    recompensas.append(recompensa_total)\n",
        "    print(f\"Episodio {ep+1}: recompensa total = {recompensa_total:.2f}\")\n",
        "\n",
        "recompensas = np.array(recompensas)\n",
        "print(f\"\\nPromedio de recompensas del agente: {recompensas.mean():.4f}\")\n",
        "print(f\"Desviaci√≥n est√°ndar: {recompensas.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el baseline aleatorio el promedio de recompensas era aproximadamente -279 con una desviaci√≥n est√°ndar cercana a 194, mientras que el agente entrenado obtiene un promedio cercano a -156 con una desviaci√≥n est√°ndar alrededor de 93. Es decir, las recompensas son menos negativas (mejor desempe√±o promedio) y adem√°s hay menos variabilidad entre episodios (comportamiento m√°s estable).\n",
        "\n",
        "Aun as√≠, como el promedio sigue siendo claramente negativo y varios episodios terminan con malas recompensas, se puede concluir que el agente mejora respecto al baseline, pero todav√≠a no aprende una pol√≠tica realmente buena para el ambiente."
      ],
      "metadata": {
        "id": "HtKRUlhTr5PW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Xw4YHT3P5d"
      },
      "source": [
        "#### **1.2.5 Optimizaci√≥n de modelo (0.2 puntos)**\n",
        "\n",
        "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par√°metros como:\n",
        "- `total_timesteps`\n",
        "- `learning_rate`\n",
        "- `batch_size`\n",
        "\n",
        "Una vez optimizado el modelo, use la funci√≥n `export_gif` para estudiar el comportamiento de su agente en la resoluci√≥n del ambiente y comente sobre sus resultados.\n",
        "\n",
        "Adjunte el gif generado en su entrega (mejor a√∫n si adem√°s adjuntan el gif en el markdown)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aItYF6sr6F_6"
      },
      "outputs": [],
      "source": [
        "policy_kwargs = dict(net_arch=[256, 256])\n",
        "model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    learning_rate=1e-4,     # m√°s peque√±o\n",
        "    batch_size=1024,\n",
        "    n_steps=4096,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    clip_range=0.2,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "model.learn(total_timesteps=2_000_000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 10\n",
        "recompensas = []\n",
        "\n",
        "for ep in range(n_episodes):\n",
        "    obs, info = env.reset()\n",
        "    terminado = False\n",
        "    recompensa_total = 0.0\n",
        "\n",
        "    while not terminado:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        recompensa_total += reward\n",
        "        terminado = terminated or truncated\n",
        "\n",
        "    recompensas.append(recompensa_total)\n",
        "    print(f\"Episodio {ep+1}: recompensa total = {recompensa_total:.2f}\")\n",
        "\n",
        "recompensas = np.array(recompensas)\n",
        "print(f\"\\nPromedio de recompensas del agente: {recompensas.mean():.4f}\")\n",
        "print(f\"Desviaci√≥n est√°ndar: {recompensas.std():.4f}\")\n"
      ],
      "metadata": {
        "id": "YqHrchYnsjs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_gif(model, filename=\"lunarlander_agent.gif\", n_episodes=5)"
      ],
      "metadata": {
        "id": "wBa8-9eStPyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPUY-Ktgf2BO"
      },
      "source": [
        "## **2. Large Language Models (4.0 puntos)**\n",
        "\n",
        "En esta secci√≥n se enfocar√°n en habilitar un Chatbot que nos permita responder preguntas √∫tiles a trav√©s de LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ4fPRRihGLe"
      },
      "source": [
        "### **2.0 Configuraci√≥n Inicial**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Como siempre, cargamos todas nuestras API KEY al entorno:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers langchain-huggingface"
      ],
      "metadata": {
        "id": "YPl_8t3_2ZVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "j_QMfsn72aI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj9JvQUsgZZJ"
      },
      "source": [
        "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "El objetivo de esta subsecci√≥n es que habiliten un chatbot que pueda responder preguntas usando informaci√≥n contenida en documentos PDF a trav√©s de **Retrieval Augmented Generation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxOQroVnaZ5"
      },
      "source": [
        "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
        "\n",
        "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
        "  - 2 documentos .pdf como m√≠nimo.\n",
        "  - 50 p√°ginas de contenido como m√≠nimo entre todos los documentos.\n",
        "  - Ideas para documentos: Documentos relacionados a temas acad√©micos, laborales o de ocio. Aprovechen este ejercicio para construir algo √∫til y/o relevante para ustedes!\n",
        "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
        "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
        "  - **Recuerden adjuntar los documentos en su entrega**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D1tIRCi4oJJ"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "for doc in doc_paths:\n",
        "    print(f\"Revisando: {doc}\")\n",
        "    try:\n",
        "        reader = PyPDF2.PdfReader(open(doc, \"rb\"))\n",
        "        print(\"  OK, p√°ginas:\", len(reader.pages))\n",
        "    except Exception as e:\n",
        "        print(\"  ERROR:\", e)\n"
      ],
      "metadata": {
        "id": "llpgRrMq7gB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzq2TjWCnu15"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "doc_paths = [\n",
        "    \"/content/doc 1 (mitologia griega).pdf\",\n",
        "    \"/content/Reglamento sobre el Estatuto y la Transferencia de Jugadores - EdiciSn de enero de 2025.pdf\",\n",
        "]\n",
        "\n",
        "assert len(doc_paths) >= 2, \"Deben adjuntar un m√≠nimo de 2 documentos\"\n",
        "\n",
        "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
        "assert total_paginas >= 50, f\"P√°ginas insuficientes: {total_paginas}\"\n",
        "print(\"Total de p√°ginas:\", total_paginas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r811-P71nizA"
      },
      "source": [
        "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
        "\n",
        "Vectorice los documentos y almacene sus representaciones de manera acorde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-yXAdCSn4JM"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "def pdf_to_text(path):\n",
        "    reader = PyPDF2.PdfReader(open(path, \"rb\"))\n",
        "    pages = [page.extract_text() or \"\" for page in reader.pages]\n",
        "    return \"\\n\".join(pages)\n",
        "\n",
        "raw_docs = []\n",
        "for path in doc_paths:\n",
        "    text = pdf_to_text(path)\n",
        "    raw_docs.append({\"text\": text, \"source\": path})\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # tama√±o de cada trozo de texto\n",
        "    chunk_overlap=200,    # cu√°nto se solapan\n",
        ")\n",
        "\n",
        "texts = []\n",
        "metadatas = []\n",
        "\n",
        "for d in raw_docs:\n",
        "    chunks = text_splitter.split_text(d[\"text\"])\n",
        "    texts.extend(chunks)\n",
        "    metadatas.extend([{\"source\": d[\"source\"]}] * len(chunks))\n",
        "\n",
        "print(\"N¬∞ de chunks:\", len(texts))\n",
        "\n",
        "vectorstore = FAISS.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=embeddings,\n",
        "    metadatas=metadatas,\n",
        ")\n",
        "\n",
        "\n",
        "vectorstore.save_local(\"indice_mitos_fifa\")\n",
        "\n",
        "print(\"√çndice guardado en carpeta: indice_mitos_fifa\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAUkP5zrnyBK"
      },
      "source": [
        "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
        "\n",
        "Habilite la soluci√≥n RAG a trav√©s de una *chain* y gu√°rdela en una variable."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai\n"
      ],
      "metadata": {
        "id": "jq2VpODOLBYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Pega aqu√≠ tu API key (no la muestres en el chat)\n",
        "client = OpenAI(api_key=\"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\")\n"
      ],
      "metadata": {
        "id": "iO0UTREFLBwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRAGChain:\n",
        "    def __init__(self, vectorstore, client):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.client = client\n",
        "\n",
        "    def invoke(self, inputs, k: int = 4):\n",
        "        # Soportar tanto {\"input\": \"...\"} como {\"query\": \"...\"} o un string directo\n",
        "        if isinstance(inputs, dict):\n",
        "            query = inputs.get(\"input\") or inputs.get(\"query\") or \"\"\n",
        "        else:\n",
        "            query = str(inputs)\n",
        "\n",
        "        # 1) Recuperar documentos relevantes\n",
        "        docs = self.vectorstore.similarity_search(query, k=k)\n",
        "        contexto = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "        # 2) Construir el prompt con contexto + pregunta\n",
        "        prompt = f\"\"\"\n",
        "Eres un asistente que responde SOLO usando la informaci√≥n de los documentos\n",
        "de mitolog√≠a griega y del Reglamento sobre el Estatuto y la Transferencia de Jugadores de la FIFA.\n",
        "\n",
        "Si la respuesta NO est√° en el contexto, responde claramente que no tienes informaci√≥n suficiente.\n",
        "\n",
        "Contexto:\n",
        "{contexto}\n",
        "\n",
        "Pregunta:\n",
        "{query}\n",
        "\n",
        "Responde en espa√±ol, de forma clara y concisa.\n",
        "\"\"\"\n",
        "\n",
        "        # 3) Llamar al modelo de OpenAI (Responses API)\n",
        "        response = self.client.responses.create(\n",
        "            model=\"gpt-4o-mini\",  # puedes cambiar el modelo si quieres\n",
        "            input=prompt,\n",
        "        )\n",
        "\n",
        "        answer = response.output_text\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": answer,\n",
        "            \"source_documents\": docs,\n",
        "        }\n",
        "\n",
        "# ‚úÖ Aqu√≠ habilitas la soluci√≥n RAG como \"chain\" y la guardas en una variable\n",
        "rag_chain = SimpleRAGChain(vectorstore, client)\n"
      ],
      "metadata": {
        "id": "JfPX-ZuwLB1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRAGChain:\n",
        "    def __init__(self, vectorstore):\n",
        "        self.vectorstore = vectorstore\n",
        "\n",
        "    def invoke(self, inputs, k: int = 4):\n",
        "        # Soportar {\"input\": \"...\"} o {\"query\": \"...\"} o string directo\n",
        "        if isinstance(inputs, dict):\n",
        "            query = inputs.get(\"input\") or inputs.get(\"query\") or \"\"\n",
        "        else:\n",
        "            query = str(inputs)\n",
        "\n",
        "        # 1) Recuperar documentos relevantes\n",
        "        docs = self.vectorstore.similarity_search(query, k=k)\n",
        "        contexto = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "        # 2) \"Respuesta\" simulada: devolvemos un recorte del contexto\n",
        "        answer = (\n",
        "            \"Por falta de cuota en la API no se pudo llamar al modelo de lenguaje.\\n\"\n",
        "            \"A continuaci√≥n se muestra un extracto del contexto m√°s relevante recuperado:\\n\\n\"\n",
        "            + contexto[:700]  # primeros 700 caracteres\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": answer,\n",
        "            \"source_documents\": docs,\n",
        "        }\n",
        "\n",
        "# Ahora creas tu chain as√≠:\n",
        "rag_chain = SimpleRAGChain(vectorstore)\n"
      ],
      "metadata": {
        "id": "0k3acBDUNB7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycg5S5i_n-kL"
      },
      "source": [
        "#### **2.1.4 Verificaci√≥n de respuestas (0.5 puntos)**\n",
        "\n",
        "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci√≥n para cada una. ¬øSu soluci√≥n RAG entrega las respuestas que esperaba?\n",
        "\n",
        "Ejemplo de tupla:\n",
        "- Pregunta: ¬øQui√©n es el presidente de Chile?\n",
        "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listado de 3 tuplas (pregunta, respuesta_correcta)\n",
        "qa_eval = [\n",
        "    (\n",
        "        \"¬øQui√©n es Zeus y qu√© papel cumple en la mitolog√≠a griega?\",\n",
        "        \"Zeus es el dios supremo del pante√≥n griego, hijo de Cronos y Rea. \"\n",
        "        \"Es el dios del cielo y del trueno y gobierna a los dem√°s dioses desde el monte Olimpo.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Menciona al menos dos s√≠mbolos o atributos caracter√≠sticos de Atenea en la mitolog√≠a griega.\",\n",
        "        \"Atenea es la diosa de la sabidur√≠a y la guerra estrat√©gica. Sus s√≠mbolos m√°s caracter√≠sticos son el b√∫ho y el olivo, \"\n",
        "        \"y suele representarse con casco, lanza y √©gida.\"\n",
        "    ),\n",
        "    (\n",
        "        \"¬øEn qu√© condiciones la normativa FIFA permite una transferencia internacional de un jugador menor de 18 a√±os?\",\n",
        "        \"La normativa FIFA solo permite transferencias internacionales de menores en casos excepcionales, como cuando los padres del jugador \"\n",
        "        \"se mudan al pa√≠s por razones ajenas al f√∫tbol, cuando la transferencia ocurre dentro de la Uni√≥n Europea o el EEE entre los 16 y 18 a√±os, \"\n",
        "        \"o cuando el jugador vive cerca de una frontera y el club est√° dentro de un radio determinado.\"\n",
        "    ),\n",
        "]\n"
      ],
      "metadata": {
        "id": "FRv0ZBS0No9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (pregunta, respuesta_correcta) in enumerate(qa_eval, 1):\n",
        "    # Llamar a la soluci√≥n RAG\n",
        "    result = rag_chain.invoke({\"input\": pregunta})\n",
        "\n",
        "    # Intentar obtener el texto de respuesta seg√∫n el tipo de chain\n",
        "    if isinstance(result, dict):\n",
        "        respuesta_modelo = result.get(\"answer\") or result.get(\"result\") or str(result)\n",
        "    else:\n",
        "        respuesta_modelo = str(result)\n",
        "\n",
        "    print(f\"\\n================ PREGUNTA {i} ================\")\n",
        "    print(\"Pregunta:\")\n",
        "    print(pregunta)\n",
        "\n",
        "    print(\"\\nRespuesta correcta esperada:\")\n",
        "    print(respuesta_correcta)\n",
        "\n",
        "    print(\"\\nRespuesta de la soluci√≥n RAG:\")\n",
        "    print(respuesta_modelo[:1000], \"...\")\n",
        "    print(\"\\n(Desde aqu√≠ t√∫ comentas en el informe si la respuesta es correcta, parcial o incorrecta.)\")\n",
        "    print(\"===============================================\")\n"
      ],
      "metadata": {
        "id": "POKheeF8NpzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8d5zTMHoUgF"
      },
      "source": [
        "#### **2.1.5 Sensibilidad de Hiperpar√°metros (0.5 puntos)**\n",
        "\n",
        "Extienda el an√°lisis del punto 2.1.4 analizando c√≥mo cambian las respuestas entregadas cambiando los siguientes hiperpar√°metros:\n",
        "- `Tama√±o del chunk`. (*¬øC√≥mo repercute que los chunks sean mas grandes o chicos?*)\n",
        "- `La cantidad de chunks recuperados`. (*¬øQu√© pasa si se devuelven muchos/pocos chunks?*)\n",
        "- `El tipo de b√∫squeda`. (*¬øC√≥mo afecta el tipo de b√∫squeda a las respuestas de mi RAG?*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDh_QgeXLGHc"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 2.1.5 Sensibilidad de Hiperpar√°metros\n",
        "# =====================================================\n",
        "\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# --- 1) Volvemos a leer los PDFs una sola vez en bruto ---\n",
        "\n",
        "def pdf_to_text(path):\n",
        "    reader = PyPDF2.PdfReader(open(path, \"rb\"))\n",
        "    pages = [page.extract_text() or \"\" for page in reader.pages]\n",
        "    return \"\\n\".join(pages)\n",
        "\n",
        "raw_docs = []\n",
        "for path in doc_paths:\n",
        "    text = pdf_to_text(path)\n",
        "    raw_docs.append({\"text\": text, \"source\": path})\n",
        "\n",
        "print(\"Documentos le√≠dos para an√°lisis de sensibilidad.\")\n",
        "\n",
        "\n",
        "# --- 2) Funci√≥n para construir un vectorstore con distinto chunk_size ---\n",
        "\n",
        "def build_vectorstore(chunk_size):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=200,\n",
        "    )\n",
        "\n",
        "    texts = []\n",
        "    metadatas = []\n",
        "\n",
        "    for d in raw_docs:\n",
        "        chunks = text_splitter.split_text(d[\"text\"])\n",
        "        texts.extend(chunks)\n",
        "        metadatas.extend([{\"source\": d[\"source\"]}] * len(chunks))\n",
        "\n",
        "    print(f\"chunk_size = {chunk_size}, N¬∞ chunks = {len(texts)}\")\n",
        "\n",
        "    vs = FAISS.from_texts(\n",
        "        texts=texts,\n",
        "        embedding=embeddings,\n",
        "        metadatas=metadatas,\n",
        "    )\n",
        "    return vs\n",
        "\n",
        "\n",
        "# --- 3) Construimos dos √≠ndices: uno con chunks chicos y otro con chunks grandes ---\n",
        "\n",
        "vectorstore_small = build_vectorstore(chunk_size=500)\n",
        "vectorstore_large = build_vectorstore(chunk_size=2000)\n",
        "\n",
        "# Tambi√©n podemos usar el vectorstore \"original\" que ya ten√≠an creado antes\n",
        "# (con chunk_size=1000) para algunos experimentos:\n",
        "vectorstore_base = vectorstore  # alias solo para claridad\n",
        "\n",
        "\n",
        "# --- 4) Definimos una chain RAG simple (simulada, sin LLM) ---\n",
        "\n",
        "class SimpleRAGChain:\n",
        "    def __init__(self, vectorstore):\n",
        "        self.vectorstore = vectorstore\n",
        "\n",
        "    def invoke(self, inputs, k: int = 4):\n",
        "        # Soporta {\"input\": \"...\"} o {\"query\": \"...\"} o string directo\n",
        "        if isinstance(inputs, dict):\n",
        "            query = inputs.get(\"input\") or inputs.get(\"query\") or \"\"\n",
        "        else:\n",
        "            query = str(inputs)\n",
        "\n",
        "        # Recuperar documentos relevantes\n",
        "        docs = self.vectorstore.similarity_search(query, k=k)\n",
        "        contexto = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "        # \"Respuesta\" simulada: mostramos parte del contexto\n",
        "        answer = (\n",
        "            \"Respuesta simulada (sin modelo de lenguaje).\\n\"\n",
        "            \"Extracto del contexto m√°s relevante recuperado:\\n\\n\"\n",
        "            + contexto[:700]\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": answer,\n",
        "            \"source_documents\": docs,\n",
        "        }\n",
        "\n",
        "\n",
        "# Creamos chains para el √≠ndice con chunks peque√±os y grandes\n",
        "rag_small = SimpleRAGChain(vectorstore_small)\n",
        "rag_large = SimpleRAGChain(vectorstore_large)\n",
        "\n",
        "# Y una chain base usando el √≠ndice original\n",
        "rag_base = SimpleRAGChain(vectorstore_base)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# A) Sensibilidad al TAMA√ëO DE CHUNK\n",
        "# =====================================================\n",
        "\n",
        "pregunta_chunk = \"¬øQui√©n es Zeus y qu√© pape_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENJiPPM0giX8"
      },
      "source": [
        "### **2.2 Agentes (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la secci√≥n anterior, en esta secci√≥n se busca habilitar **Agentes** para obtener informaci√≥n a trav√©s de tools y as√≠ responder la pregunta del usuario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V47l7Mjfrk0N"
      },
      "source": [
        "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
        "\n",
        "Generar una *tool* que pueda hacer consultas al motor de b√∫squeda **Tavily**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6SLKwcWr0AG"
      },
      "outputs": [],
      "source": [
        "# Tool para consultar al motor de b√∫squeda Tavily\n",
        "\n",
        "!pip install -q tavily-python langchain-core\n",
        "\n",
        "from tavily import TavilyClient\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# Ojo: NO pasamos la API key aqu√≠.\n",
        "# TavilyClient la toma autom√°ticamente de la variable de entorno TAVILY_API_KEY\n",
        "tavily_client = TavilyClient()\n",
        "\n",
        "@tool\n",
        "def tavily_search_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Realiza una b√∫squeda web usando Tavily y devuelve\n",
        "    un resumen breve de la informaci√≥n encontrada en espa√±ol.\n",
        "    √ösala cuando necesites informaci√≥n actualizada que no est√© en tus PDFs.\n",
        "    \"\"\"\n",
        "    response = tavily_client.search(\n",
        "        query=query,\n",
        "        max_results=5,         # puedes ajustar si quieres\n",
        "        search_depth=\"basic\",  # \"advanced\" si necesitas algo m√°s profundo\n",
        "    )\n",
        "    # Tavily normalmente entrega un campo 'answer' ya resumido\n",
        "    return response.get(\"answer\", \"Tavily no encontr√≥ una respuesta clara para esta consulta.\")\n",
        "tools = [tavily_search_tool]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SonB1A-9rtRq"
      },
      "source": [
        "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
        "\n",
        "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
        "\n",
        "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehJJpoqsr26-"
      },
      "outputs": [],
      "source": [
        "# Tool para hacer consultas a Wikipedia\n",
        "\n",
        "!pip install -q wikipedia langchain-core\n",
        "\n",
        "import wikipedia\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# Configuramos Wikipedia en espa√±ol\n",
        "wikipedia.set_lang(\"es\")\n",
        "\n",
        "@tool\n",
        "def wikipedia_search_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Realiza una consulta a Wikipedia y devuelve\n",
        "    un resumen breve en espa√±ol sobre el tema solicitado.\n",
        "    √ösala cuando necesites informaci√≥n general que no est√© en tus documentos.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resumen = wikipedia.summary(query, sentences=3)  # primeras ~3 frases\n",
        "        return resumen\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        # Si hay muchas posibles p√°ginas, devolvemos algunas opciones\n",
        "        opciones = \", \".join(e.options[:5])\n",
        "        return (\n",
        "            \"La consulta es ambigua en Wikipedia. Algunas opciones posibles son: \"\n",
        "            + opciones\n",
        "        )\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        return \"No encontr√© una p√°gina de Wikipedia para esa consulta.\"\n",
        "    except Exception as e:\n",
        "        return f\"Ocurri√≥ un error al consultar Wikipedia: {e}\"\n",
        "tools = [wikipedia_search_tool]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUIMdX6r0ne"
      },
      "source": [
        "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
        "\n",
        "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg√∫rese que su agente responda en espa√±ol. Por √∫ltimo, guarde el agente en una variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD1_n0wrsDI5"
      },
      "outputs": [],
      "source": [
        "# Crear un agente que use las tools de Tavily y Wikipedia\n",
        "\n",
        "!pip install -qU langchain\n",
        "\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"gpt-4.1-mini\",   # o el modelo que el entorno del curso tenga configurado\n",
        "    tools=[tavily_search_tool, wikipedia_search_tool],\n",
        "    system_prompt=(\n",
        "        \"Eres un asistente √∫til que SIEMPRE responde en espa√±ol. \"\n",
        "        \"Usa las tools disponibles (b√∫squeda en Tavily y consultas a Wikipedia) \"\n",
        "        \"para obtener informaci√≥n actualizada y precisa cuando sea necesario. \"\n",
        "        \"Si no puedes responder con seguridad, dilo expl√≠citamente.\"\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "respuesta = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"¬øQui√©n fue Atenea en la mitolog√≠a griega?\"}]}\n",
        ")\n",
        "print(respuesta)\n"
      ],
      "metadata": {
        "id": "N_H6qvxlPF9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKV0JxK3r-XG"
      },
      "source": [
        "#### **2.2.4 Verificaci√≥n de respuestas (0.3 puntos)**\n",
        "\n",
        "Pruebe el funcionamiento de su agente y aseg√∫rese que el agente est√© ocupando correctamente las tools disponibles. ¬øEn qu√© casos el agente deber√≠a ocupar la tool de Tavily? ¬øEn qu√© casos deber√≠a ocupar la tool de Wikipedia?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqo2dsxvywW_"
      },
      "outputs": [],
      "source": [
        "# Probamos el funcionamiento b√°sico del agente\n",
        "\n",
        "preguntas = [\n",
        "    \"¬øQui√©n fue Atenea en la mitolog√≠a griega?\",\n",
        "    \"¬øQu√© es la FIFA y a qu√© se dedica?\",\n",
        "    \"¬øCu√°l es la capital de Jap√≥n?\"\n",
        "]\n",
        "\n",
        "for i, q in enumerate(preguntas, 1):\n",
        "    print(f\"\\n===== PREGUNTA {i} =====\")\n",
        "    print(\"Usuario:\", q)\n",
        "\n",
        "    # El agente sigue el formato de la documentaci√≥n: messages = [...]\n",
        "    result = agent.invoke(\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": q}\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Dependiendo de la versi√≥n, result suele traer una lista de messages\n",
        "    messages = result.get(\"messages\", [])\n",
        "    if messages:\n",
        "        respuesta_modelo = messages[-1][\"content\"]\n",
        "    else:\n",
        "        respuesta_modelo = result\n",
        "\n",
        "    print(\"\\nRespuesta del agente:\\n\")\n",
        "    print(respuesta_modelo)\n",
        "    print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZbDTYiogquv"
      },
      "source": [
        "### **2.3 Multi Agente (1.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
        "\" width=\"450\">\n",
        "</p>\n",
        "\n",
        "El objetivo de esta subsecci√≥n es encapsular las funcionalidades creadas en una soluci√≥n multiagente con un **supervisor**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-iUfH0WvI6m"
      },
      "source": [
        "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
        "\n",
        "Transforme la soluci√≥n RAG de la secci√≥n 2.1 y el agente de la secci√≥n 2.2 a *tools* (una tool por cada uno)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw1cfTtvv1AZ"
      },
      "outputs": [],
      "source": [
        "# Tools que encapsulan la soluci√≥n RAG y el agente web\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def rag_qa_tool(pregunta: str) -> str:\n",
        "    \"\"\"\n",
        "    Usa la soluci√≥n RAG de la secci√≥n 2.1 para responder preguntas\n",
        "    utilizando los documentos PDF (mitolog√≠a griega + reglamento FIFA).\n",
        "    \"\"\"\n",
        "    # Asumimos que rag_chain ya est√° definido (SimpleRAGChain de 2.1)\n",
        "    result = rag_chain.invoke({\"input\": pregunta})\n",
        "\n",
        "    if isinstance(result, dict):\n",
        "        # En nuestra implementaci√≥n de SimpleRAGChain usamos la clave \"answer\"\n",
        "        answer = result.get(\"answer\") or result.get(\"result\") or str(result)\n",
        "    else:\n",
        "        answer = str(result)\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "@tool\n",
        "def web_agent_tool(pregunta: str) -> str:\n",
        "    \"\"\"\n",
        "    Usa el agente de la secci√≥n 2.2 (con Tavily y Wikipedia)\n",
        "    para responder preguntas generales usando herramientas externas.\n",
        "    Responde siempre en espa√±ol.\n",
        "    \"\"\"\n",
        "    # Asumimos que agent ya est√° definido (create_agent(...))\n",
        "    result = agent.invoke(\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": pregunta}\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Dependiendo de la versi√≥n, el agente devuelve un dict con \"messages\"\n",
        "    if isinstance(result, dict) and \"messages\" in result:\n",
        "        messages = result[\"messages\"]\n",
        "        if messages:\n",
        "            # √öltimo mensaje del agente suele ser la respuesta final\n",
        "            return messages[-1][\"content\"]\n",
        "\n",
        "    # Fallback por si el formato es distinto\n",
        "    return str(result)\n",
        "\n",
        "\n",
        "# (Opcional) Lista de tools para el supervisor multiagente\n",
        "supervisor_tools = [rag_qa_tool, web_agent_tool]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYNjT_0vPCg"
      },
      "source": [
        "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
        "\n",
        "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv2ZY0BAv1RD"
      },
      "outputs": [],
      "source": [
        "# Agente supervisor que decide cu√°ndo usar RAG o el agente web\n",
        "\n",
        "!pip install -qU langchain\n",
        "\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "supervisor = create_agent(\n",
        "    model=\"gpt-4.1-mini\",  # o el modelo que tengan configurado en el curso\n",
        "    tools=[rag_qa_tool, web_agent_tool],\n",
        "    system_prompt=(\n",
        "        \"Eres un supervisor multiagente que SIEMPRE responde en espa√±ol.\\n\"\n",
        "        \"Tienes acceso a dos tools:\\n\"\n",
        "        \"- rag_qa_tool: usa documentos PDF de mitolog√≠a griega y reglamento FIFA.\\n\"\n",
        "        \"- web_agent_tool: usa herramientas externas (Tavily / Wikipedia) para buscar en la web.\\n\"\n",
        "        \"Elige la tool m√°s adecuada seg√∫n la pregunta del usuario. \"\n",
        "        \"Si la pregunta est√° cubierta por los documentos, prioriza rag_qa_tool. \"\n",
        "        \"Si requiere informaci√≥n actualizada o fuera de los PDFs, usa web_agent_tool.\\n\"\n",
        "        \"Si no puedes responder con seguridad, dilo expl√≠citamente.\"\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3zWlvyvY7K"
      },
      "source": [
        "#### **2.3.3 Verificaci√≥n de respuestas (0.25 puntos)**\n",
        "\n",
        "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¬øC√≥mo var√≠an las respuestas bajo este enfoque?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_1t0zkgv1qW"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Prueba del agente supervisor\n",
        "# ================================\n",
        "\n",
        "# Preguntas de la secci√≥n 2.1.4 (RAG sobre PDFs)\n",
        "preguntas_214 = [\n",
        "    \"¬øQui√©n es Zeus y qu√© papel cumple en la mitolog√≠a griega?\",\n",
        "    \"Menciona al menos dos s√≠mbolos o atributos caracter√≠sticos de Atenea en la mitolog√≠a griega.\",\n",
        "    \"¬øEn qu√© condiciones la normativa FIFA permite una transferencia internacional de un jugador menor de 18 a√±os?\",\n",
        "]\n",
        "\n",
        "# Preguntas de la secci√≥n 2.2.4 (agente con tools externas)\n",
        "preguntas_224 = [\n",
        "    \"¬øQui√©n fue Atenea en la mitolog√≠a griega?\",\n",
        "    \"¬øQu√© es la FIFA y a qu√© se dedica?\",\n",
        "    \"¬øCu√°l es la capital de Jap√≥n?\",\n",
        "]\n",
        "\n",
        "def preguntar_supervisor(pregunta: str):\n",
        "    \"\"\"Env√≠a una pregunta al supervisor y devuelve el texto de la respuesta.\"\"\"\n",
        "    result = supervisor.invoke(\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": pregunta}\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if isinstance(result, dict) and \"messages\" in result:\n",
        "        messages = result[\"messages\"]\n",
        "        if messages:\n",
        "            return messages[-1][\"content\"]\n",
        "    return str(result)\n",
        "\n",
        "\n",
        "print(\"========== PREGUNTAS SECCI√ìN 2.1.4 (RAG) ==========\\n\")\n",
        "for i, q in enumerate(preguntas_214, 1):\n",
        "    print(f\"PREGUNTA 2.1.4-{i}: {q}\\n\")\n",
        "    respuesta = preguntar_supervisor(q)\n",
        "    print(\"Respuesta del supervisor:\\n\")\n",
        "    print(respuesta)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "print(\"========== PREGUNTAS SECCI√ìN 2.2.4 (AGENTE WEB) ==========\\n\")\n",
        "for i, q in enumerate(preguntas_224, 1):\n",
        "    print(f\"PREGUNTA 2.2.4-{i}: {q}\\n\")\n",
        "    respuesta = preguntar_supervisor(q)\n",
        "    print(\"Respuesta del supervisor:\\n\")\n",
        "    print(respuesta)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb8bdAmYvgwn"
      },
      "source": [
        "#### **2.3.4 An√°lisis (0.25 puntos)**\n",
        "\n",
        "¬øQu√© diferencias tiene este enfoque con la soluci√≥n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAUlJxqoLK5r"
      },
      "source": [
        "`escriba su respuesta ac√°`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JWVSuWiZ8Mj"
      },
      "source": [
        "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
        "\n",
        "- Pregunta 1: \"Hola! mi nombre es Sebasti√°n\"\n",
        "  - Respuesta esperada: \"Hola Sebasti√°n! ...\"\n",
        "- Pregunta 2: \"Cual es mi nombre?\"\n",
        "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
        "  - **Respuesta esperada: \"Tu nombre es Sebasti√°n\"**\n",
        "\n",
        "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci√≥n entregada en el punto 2.3.\n",
        "\n",
        "**Nota: El Bonus es v√°lido <u>s√≥lo para la secci√≥n 2 de Large Language Models.</u>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Y7tIPJLPfB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFc3jBT5g0kT"
      },
      "source": [
        "### **2.5 Despliegue (0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav√©s de `gradio`, una librer√≠a especializada en el levantamiento r√°pido de demos basadas en ML.\n",
        "\n",
        "Primero instalamos la librer√≠a:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8TsvnCPbkIA"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJBztEUovKsF"
      },
      "source": [
        "Luego s√≥lo deben ejecutar el siguiente c√≥digo e interactuar con la interfaz a trav√©s del notebook o del link generado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3KedQSvg1-n"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "def agent_response(message, history):\n",
        "  '''\n",
        "  Funci√≥n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
        "  '''\n",
        "  # get chatbot response\n",
        "  response = ... # rellenar con la respuesta de su chat\n",
        "\n",
        "  # assert\n",
        "  assert type(response) == str, \"output de route_question debe ser string\"\n",
        "\n",
        "  # \"streaming\" response\n",
        "  for i in range(len(response)):\n",
        "    time.sleep(0.015)\n",
        "    yield response[: i+1]\n",
        "\n",
        "gr.ChatInterface(\n",
        "    agent_response,\n",
        "    type=\"messages\",\n",
        "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
        "    description=\"Hola! Soy un chatbot muy √∫til :)\", # tambi√©n la descripci√≥n\n",
        "    theme=\"soft\",\n",
        "    ).launch(\n",
        "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
        "        debug = False,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq_HN9WPddaB"
      },
      "source": [
        "# Conclusi√≥n\n",
        "√âxito!\n",
        "<center>\n",
        "<img src =\"https://media.tenor.com/MRQgxcelAV8AAAAM/perry-the-platypus-phineas-and-ferb.gif\" width = 400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12856c05"
      },
      "source": [
        "# Task\n",
        "Simulate 5000 episodes of the Blackjack game using a random action policy, then calculate and report the mean and standard deviation of the rewards, and interpret the performance of this random policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "106d3f18"
      },
      "source": [
        "## Simulate Random Actions\n",
        "\n",
        "### Subtask:\n",
        "Simulate 5000 episodes in the Blackjack environment, taking random actions in each step, and collect the total reward for each episode.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5ffa15b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires simulating 5000 episodes of Blackjack with random actions and collecting the total reward for each episode. The provided instructions detail the exact steps to achieve this, including initializing a reward list, looping through episodes, resetting the environment, taking random actions, accumulating rewards, and storing them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7116151"
      },
      "source": [
        "episode_rewards = []\n",
        "\n",
        "for i in range(5000):\n",
        "    observation, info = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample() # take a random action\n",
        "        observation, reward, done, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "    episode_rewards.append(total_reward)\n",
        "\n",
        "print(\"Simulation complete. Collected rewards for 5000 episodes.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}